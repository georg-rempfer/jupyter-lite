{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Description\n\nThis notebook calculates a one-sided binomial confidence interval for the single trial success probability given the number of tests conducted ```n_trials```, the number of successful tests ```n_successes``` out of these, and a desired ```confidence_level``` in this success probability.\n\nWhen faced with test results, this tells you what you should expect the real single trial success probability (if you conducted an infinite number of tests) to be. You can tune how conservative or optimistic this estimate for the real single trial success probability should be by setting the ```confidence level```. If you set the confidence level to ```0.95```, the real single trial success probability will lie within the interval calculated at least in ```95%``` of these evaluations. That means there still is at most a ```5%``` chance that the real single trial success probability will be worse than the estimated interval suggests.\n\n__Note__\n1. All probabilities are given as numbers between 0 and 1, where 0 represents \"never\" and 1 represents 100% certainty.\n2. The confidence intervals are conservative, i.e., the coverage of this hypothesis test is ```confidence_level``` or higher. That means if you use this notebook with ```confidence_level = 0.95``` for multiple trials, then you should expect the true single trial success rate to lie outside the reported confidence interval in 1/20 times on average in the worst-case.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Parameters",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n_trials = 100 #number of trials conducted\nn_successes = 92 #number of successful trials\nconfidence_level = 0.95\n\nround_to_digits = 4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "# Evaluation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport mpmath as mp\nimport numpy as np\nimport scipy.optimize as opt\n\nmp.mp.dps = 50 #use 50 decimal digits precision in mpmath calculations\n\ndef binomial_distr(n_successes, p_success, n_trials):\n    return mp.binomial(n_trials, n_successes) * mp.power(mp.mpf(p_success), n_successes) * mp.power(mp.mpf(1-p_success), (n_trials-n_successes))\n\nbinomial_distr_vectorized = np.vectorize(binomial_distr) #create version of binomial distribution that can be evaluated for whole lists of inputs\n\ndef binomial_confidence(n_successes, p_success_hypothesis, n_trials):\n    return float(1-binomial_distr_vectorized(np.arange(n_successes, n_trials+1), p_success_hypothesis, n_trials).sum())\n    \np_min = opt.bisect(lambda p: binomial_confidence(n_successes, p, n_trials)-confidence_level, 0, 1)\np_min = mp.nstr(mp.mpf(p_min), round_to_digits)\n\nprint('The single trial success probability p_success lies within [{}, 1] with a confidence of {}'.format(p_min, confidence_level))",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The single trial success probability p_success lies within [0.8603, 1] with a confidence of 0.95\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    }
  ]
}